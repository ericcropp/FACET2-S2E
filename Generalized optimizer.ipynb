{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81bc3034-3eb1-4b18-8163-3edc7e4c3f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from UTILITY_quickstart import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a830120-07b1-4190-9adc-a8cf1aef42d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importedDefaultSettings = loadConfig(\"setLattice_configs/2024-10-14_twoBunch.yml\")\n",
    "importedDefaultSettings = loadConfig(\"setLattice_configs/2024-10-22_oneBunch.yml\")\n",
    "#importedDefaultSettings = loadConfig(\"setLattice_configs/2024-10-22_oneBunch_twoIslandStudy.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54196758-664c-49da-8a26-5f6edb549c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment set to:  /Users/nmajik/Documents/SLAC/FACET2-Bmad-PyTao\n",
      "Tracking to end\n",
      "CSR on\n",
      "Overwriting lattice with setLattice() defaults\n",
      "No defaults file provided to setLattice(). Using setLattice_configs/defaults.yml\n",
      "Number of macro particles = 10000.0\n",
      "Beam created, written to /tmp/beams/activeBeamFile_13752427244330686569.h5, and reinit to tao\n",
      "New charge: 1.600000000000001e-09\n"
     ]
    }
   ],
   "source": [
    "csrTF = True\n",
    "evalElement = \"PENT\"\n",
    "\n",
    "\n",
    "\n",
    "inputBeamFilePathSuffix = importedDefaultSettings[\"inputBeamFilePathSuffix\"]\n",
    "bunchCount = importedDefaultSettings[\"bunchCount\"]\n",
    "tao = initializeTao(\n",
    "    #inputBeamFilePathSuffix = '/beams/nmmToL0AFEND_2bunch_2024-02-16Clean/2024-02-16_2bunch_1e5Downsample_nudgeWeights.h5',\n",
    "    inputBeamFilePathSuffix = inputBeamFilePathSuffix,\n",
    "    \n",
    "    csrTF = csrTF,\n",
    "    numMacroParticles=1e4,\n",
    "    scratchPath = \"/tmp\",\n",
    "    randomizeFileNames = True\n",
    ")\n",
    "\n",
    "#Rescale charge\n",
    "# newCharge = 1e-12\n",
    "# trackBeam(tao, trackEnd = \"L0BFEND\", verbose = True)\n",
    "# P = getBeamAtElement(tao, \"L0AFEND\")\n",
    "# print(f\"\"\"\\n\\n\\nAs imported charge: {P.charge}\"\"\")\n",
    "# P.charge = newCharge\n",
    "# makeBeamActiveBeamFile(P)\n",
    "# tao.cmd('reinit beam')\n",
    "\n",
    "#Set aside the initial beam for later reference\n",
    "trackBeam(tao, trackEnd = \"L0BFEND\")\n",
    "PInit = ParticleGroup(data=tao.bunch_data(\"L0AFEND\"))\n",
    "print(f\"\"\"New charge: {PInit.charge}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4958339-61a6-442c-9213-a99470a64549",
   "metadata": {},
   "outputs": [],
   "source": [
    "targetBunchSpacing = 150e-6\n",
    "\n",
    "#masterToleranceScaling = 1.0 #Higher is looser; generally tighten for early sims and loosen for refinement\n",
    "masterToleranceScalingStart = 1\n",
    "masterToleranceScalingEnd = masterToleranceScalingStart\n",
    "masterToleranceScalingEvolutionSteps = 500\n",
    "masterToleranceScaling = masterToleranceScalingStart\n",
    "\n",
    "stageOneOptimization    = 0  # Disabling transverse terms and length limits, per https://docs.google.com/presentation/d/1b6WoEwmDz5cA0fm9FbbGiZoMwBcbCNNNCsD7JiFDxRc/edit#slide=id.g2f91233284b_0_0\n",
    "enableAlignmentTerms    = 0  # Enable centroid offset and angle penalties. Useful for removing final focus kickers from free parameters\n",
    "enableLongitudinalTerms = 1  # Enable bunch spacing penalty and bunch length objectives \n",
    "alignmentIsRelative     = 1 and (bunchCount == 2)  # Toggle between absolute and relative pointing. Must be absolute for single bunch or stuff breaks\n",
    "\n",
    "usingCheckpoint = True\n",
    "#checkpointElement = \"MFFF\"\n",
    "checkpointElement = \"CB1LE\" #Shortly downstream of BEGBC20; there's an intervening dipole though\n",
    "\n",
    "pbounds = {\n",
    "    # \"QA10361kG\": eval(importedDefaultSettings[\"QA10361kGBounds\"]),\n",
    "    # \"QA10371kG\": eval(importedDefaultSettings[\"QA10371kGBounds\"]),\n",
    "    # \"QE10425kG\": eval(importedDefaultSettings[\"QE10425kGBounds\"]),\n",
    "    # \"QE10441kG\": eval(importedDefaultSettings[\"QE10441kGBounds\"]),\n",
    "    # \"QE10511kG\": eval(importedDefaultSettings[\"QE10511kGBounds\"]),\n",
    "    # \"QE10525kG\": eval(importedDefaultSettings[\"QE10525kGBounds\"]),\n",
    "\n",
    "    # 'L0BPhaseSet': (-30, 30),\n",
    "    # 'L1PhaseSet': (-60, 0),\n",
    "    # 'L2PhaseSet': (-60, 0),\n",
    "    # 'L3PhaseSet': (-20, 20),\n",
    "    \n",
    "    # # 'L0BEnergyOffset': (-5e6, 5e6),\n",
    "    # # 'L1EnergyOffset': (-20e6, 20e6),\n",
    "    # 'L2EnergyOffset': (-500e6, 500e6),\n",
    "    # 'L3EnergyOffset': (-500e6, 500e6),\n",
    "\n",
    "    # \"Q1EkG\":  eval(importedDefaultSettings[\"Q1EkGBounds\"]),\n",
    "    # \"Q2EkG\":  eval(importedDefaultSettings[\"Q2EkGBounds\"]),\n",
    "    # \"Q3EkG\":  eval(importedDefaultSettings[\"Q3EkGBounds\"]),\n",
    "    # \"Q4EkG\":  eval(importedDefaultSettings[\"Q4EkGBounds\"]),\n",
    "    # \"Q5EkG\":  eval(importedDefaultSettings[\"Q5EkGBounds\"]),\n",
    "    # \"Q6EkG\":  eval(importedDefaultSettings[\"Q6EkGBounds\"]),\n",
    "    \n",
    "    \"S1ELkG\": eval(importedDefaultSettings[\"S1ELkGBounds\"]),\n",
    "    \"S2ELkG\": eval(importedDefaultSettings[\"S2ELkGBounds\"]),\n",
    "    \"S3ELkG\": eval(importedDefaultSettings[\"S3ELkGBounds\"]),\n",
    "    # \"S3ERkG\": eval(importedDefaultSettings[\"S3ERkGBounds\"]),\n",
    "    # \"S2ERkG\": eval(importedDefaultSettings[\"S2ERkGBounds\"]),\n",
    "    # \"S1ERkG\": eval(importedDefaultSettings[\"S1ERkGBounds\"]),\n",
    "\n",
    "    \"S1EL_xOffset\" : ( -0.004, 0.004 ),  \n",
    "    \"S1EL_yOffset\" : ( -0.004, 0.004 ),  \n",
    "    \"S2EL_xOffset\" : ( -0.004, 0.004 ),  \n",
    "    \"S2EL_yOffset\" : ( -0.004, 0.004 ),  \n",
    "    \"S2ER_xOffset\" : ( -0.004, 0.004 ),  \n",
    "    \"S2ER_yOffset\" : ( -0.004, 0.004 ),  \n",
    "    \"S1ER_xOffset\" : ( -0.004, 0.004 ),  \n",
    "    \"S1ER_yOffset\" : ( -0.004, 0.004 ),\n",
    "\n",
    "    # 'Q5FFkG': eval(importedDefaultSettings[\"Q5FFkGBounds\"]),\n",
    "    # 'Q4FFkG': eval(importedDefaultSettings[\"Q4FFkGBounds\"]),\n",
    "    # 'Q3FFkG': eval(importedDefaultSettings[\"Q3FFkGBounds\"]),\n",
    "    # 'Q2FFkG': eval(importedDefaultSettings[\"Q2FFkGBounds\"]),\n",
    "    # 'Q1FFkG': eval(importedDefaultSettings[\"Q1FFkGBounds\"]),\n",
    "    # 'Q0FFkG': eval(importedDefaultSettings[\"Q0FFkGBounds\"]),\n",
    "    # 'Q0DkG':  eval(importedDefaultSettings[\"Q0DkGBounds\"]),\n",
    "    # 'Q1DkG':  eval(importedDefaultSettings[\"Q1DkGBounds\"]),\n",
    "    # 'Q2DkG':  eval(importedDefaultSettings[\"Q2DkGBounds\"]),\n",
    "\n",
    "    # \"XC1FFkG\" : tuple(2 * x for x in eval(importedDefaultSettings[\"XC1FFkGBounds\"])), #2024-10-11: Extending bounds as proxy for tuning final chicane dipole strength\n",
    "    # \"XC3FFkG\" : eval(importedDefaultSettings[\"XC3FFkGBounds\"]),\n",
    "    # \"YC1FFkG\" : eval(importedDefaultSettings[\"YC1FFkGBounds\"]),\n",
    "    # \"YC2FFkG\" : eval(importedDefaultSettings[\"YC2FFkGBounds\"]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c8c357-a2a7-4e99-bdb1-4e53ed6e5393",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704ad00f-d1d2-42f6-a734-50be8ad97a45",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63ed0772-8ef8-4cdb-a890-83a3d7c5fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagateFromStartToCheckpoint(\n",
    "    checkpointElement\n",
    "): \n",
    "    makeBeamActiveBeamFile(PInit, tao = tao)\n",
    "\n",
    "    trackBeam(tao, trackStart = \"L0AFEND\", trackEnd = checkpointElement, **importedDefaultSettings)\n",
    "\n",
    "    P = ParticleGroup(data=tao.bunch_data(checkpointElement))\n",
    "    \n",
    "    makeBeamActiveBeamFile(P, tao = tao)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1e0d14-7eb3-4bcc-ba7d-0c71a5a83895",
   "metadata": {},
   "source": [
    "## Optimizer functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e8cd7a-9adb-4abd-a60d-010d2ca631a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "setLattice(tao, **importedDefaultSettings) #Set lattice to current default config\n",
    "\n",
    "if usingCheckpoint:\n",
    "    propagateFromStartToCheckpoint(checkpointElement)\n",
    "\n",
    "def rampToZero(val, thresh, scale = 1):\n",
    "    return (max(val, thresh) - thresh) / scale\n",
    "\n",
    "def rampToZeroFlip(val, thresh, scale = 1):\n",
    "    #This ensures some minimum val is reached and linearly penalizes values which are lower\n",
    "    return (max( -1 * val, -1 * thresh) + thresh) / scale\n",
    "\n",
    "\n",
    "def updateMasterToleranceScaling(totalNumEvals):\n",
    "    \"\"\"\n",
    "    I'm not sure if this is actually a good idea or not. \n",
    "    The general idea I'm aiming for is that early optimization iterations will get closer to the thresholded constraints than strictly necessary, \n",
    "    allowing later optimization to proceed with more wiggle room and less likelihood of falling off a \"cliff\".\n",
    "    Maybe it'd make more sense to make this conditional on something other than evaluation count?\n",
    "    Alternatively, what if it's useful to start with looser constraints and gradually tighten them? For something like COBYQA, it's less likely to \"overlearn\" when trying to satisfy, e.g. the first condition\n",
    "    An insane person might try using an exponentially decaying sine to try both...\n",
    "    \"\"\"\n",
    "    \n",
    "    global masterToleranceScaling\n",
    "\n",
    "    if totalNumEvals < masterToleranceScalingEvolutionSteps: \n",
    "        masterToleranceScaling = masterToleranceScalingStart + (masterToleranceScalingEnd - masterToleranceScalingStart) * totalNumEvals / masterToleranceScalingEvolutionSteps\n",
    "    else:\n",
    "        masterToleranceScaling = masterToleranceScalingEnd\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def specificOptimizer(\n",
    "    self,\n",
    "    **kwargs\n",
    "):\n",
    "\n",
    "    self.totalNumEvals += 1\n",
    "    self.displayEvals()\n",
    "\n",
    "    updateMasterToleranceScaling(self.totalNumEvals)\n",
    "\n",
    "    \n",
    "    savedData = kwargs\n",
    "    \n",
    "    badValue = -1e30  #The value returned for illegal config. Should be colossal. Double limit ~= 1e308\n",
    "    bigCost  = 1e20   #Should be large enough to dominate any \"normal\" return value but be dominated by badValue\n",
    "    \n",
    "    try: #This try block deals with bad configurations. Instead of causing the optimizer to halt we now 'except' a low value\n",
    "        setLattice(tao, **( importedDefaultSettings |  kwargs ))\n",
    "\n",
    "    except:\n",
    "        print(f\"specificOptimizer() excepted'd on setLattice()\")\n",
    "        return badValue * 5\n",
    "\n",
    "    try:\n",
    "        if usingCheckpoint: \n",
    "            trackBeam(tao, \n",
    "                      trackStart = checkpointElement, \n",
    "                      trackEnd = evalElement, \n",
    "                      **importedDefaultSettings)\n",
    "        else:\n",
    "            trackBeam(tao, \n",
    "                      trackStart = \"L0AFEND\", \n",
    "                      trackEnd = evalElement, \n",
    "                      **importedDefaultSettings)\n",
    "\n",
    "    except:\n",
    "            print(f\"specificOptimizer() excepted'd on trackBeam()\")\n",
    "            return badValue * 4\n",
    "    \n",
    "\n",
    "    if tao.bunch_params(evalElement)['n_particle_live'] < 10:\n",
    "        print(f\"specificOptimizer() got ~no particles after tracking\")\n",
    "        return badValue * 2 \n",
    "\n",
    "    \n",
    "    \n",
    "    try: \n",
    "        P = getBeamAtElement(tao, evalElement)\n",
    "        savedData = savedData | getBeamSpecs(P, targetTwiss = evalElement)  \n",
    "        savedData[\"lostChargeFraction\"] = 1 - (P.charge / PInit.charge)\n",
    "\n",
    "    except:\n",
    "        print(f\"specificOptimizer() excepted'd while getting beam and compiling savedData\")\n",
    "        return badValue\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    try:\n",
    "        if stageOneOptimization: \n",
    "            enableTransverse = 0\n",
    "            lengthLimitMultiplier = 0\n",
    "            stageOneSpacingToleranceMultiplier = 0.5\n",
    "    \n",
    "        else:\n",
    "            enableTransverse = 1\n",
    "            lengthLimitMultiplier = 1\n",
    "    \n",
    "        tolerableBeamLossFraction  = 0.02  * masterToleranceScaling\n",
    "        tolerableBunchSpacingError = 50e-6 * masterToleranceScaling * (stageOneSpacingToleranceMultiplier if stageOneOptimization else 1.0)\n",
    "        \n",
    "        tolerableBeamOffset  = 20e-6 * masterToleranceScaling #5e-6\n",
    "        tolerableAngleOffset = 20e-3 * masterToleranceScaling #5e-3\n",
    "        \n",
    "    \n",
    "        driveEmittanceThreshold   = 15e-6 * masterToleranceScaling #15 um-rad is 20 um at 50 cm beta\n",
    "        witnessEmittanceThreshold = 15e-6 * masterToleranceScaling\n",
    "    \n",
    "        driveSpotThreshold     = 20e-6 #* masterToleranceScaling\n",
    "        witnessSpotThreshold   = 20e-6 #* masterToleranceScaling\n",
    "        \n",
    "        driveLengthThreshold   = lengthLimitMultiplier * 20e-6 * masterToleranceScaling\n",
    "        witnessLengthThreshold = lengthLimitMultiplier * 20e-6 * masterToleranceScaling\n",
    "    \n",
    "        slicewiseBMAGThreshold = 1 + ( 0.2 * masterToleranceScaling ) #1.1\n",
    "        \n",
    "    \n",
    "        savedData[\"errorTerm_lostChargeFraction\"] = 1e3 * rampToZero( savedData[\"lostChargeFraction\"], tolerableBeamLossFraction, scale = 0.01)**2\n",
    "        \n",
    "        savedData[\"errorTerm_bunchSpacing\"] = (\n",
    "            enableLongitudinalTerms * 1e3 * rampToZero( abs(savedData[\"bunchSpacing\"] - targetBunchSpacing), tolerableBunchSpacingError, scale = 10e-6)**2\n",
    "            if bunchCount == 2\n",
    "            else\n",
    "            0\n",
    "        )\n",
    "\n",
    "        if alignmentIsRelative:\n",
    "            \n",
    "            savedData[\"errorTerm_transverseOffset\"] = enableAlignmentTerms * enableTransverse * 1e3 * np.mean([\n",
    "                        rampToZero(abs(savedData[\"PDrive_median_x\"]  - savedData[\"PWitness_median_x\"] ), tolerableBeamOffset, scale = 1e-6) ** 2,\n",
    "                        rampToZero(abs(savedData[\"PDrive_median_y\"]  - savedData[\"PWitness_median_y\"] ), tolerableBeamOffset, scale = 1e-6) ** 2,\n",
    "            ])\n",
    "            \n",
    "            savedData[\"errorTerm_angleOffset\"] = enableAlignmentTerms * enableTransverse * 1e3 * np.mean([\n",
    "                        rampToZero(abs(savedData[\"PDrive_median_xp\"]  - savedData[\"PWitness_median_xp\"] ), tolerableAngleOffset, scale = 100e-6) ** 2,\n",
    "                        rampToZero(abs(savedData[\"PDrive_median_yp\"]  - savedData[\"PWitness_median_yp\"] ), tolerableAngleOffset, scale = 100e-6) ** 2,\n",
    "            ])\n",
    "        \n",
    "        else:\n",
    "        \n",
    "            savedData[\"errorTerm_transverseOffset\"] = enableAlignmentTerms * enableTransverse * 1e3 * np.mean([\n",
    "                        (rampToZero(abs(savedData[\"PWitness_median_x\"]), tolerableBeamOffset, scale = 1e-6) ** 2 if bunchCount == 2 else 0),\n",
    "                        (rampToZero(abs(savedData[\"PWitness_median_y\"]), tolerableBeamOffset, scale = 1e-6) ** 2 if bunchCount == 2 else 0),\n",
    "                        rampToZero(abs(savedData[\"PDrive_median_x\"]  ), tolerableBeamOffset, scale = 1e-6) ** 2,\n",
    "                        rampToZero(abs(savedData[\"PDrive_median_y\"]  ), tolerableBeamOffset, scale = 1e-6) ** 2,\n",
    "            ])\n",
    "            \n",
    "            savedData[\"errorTerm_angleOffset\"] = enableAlignmentTerms * enableTransverse * 1e3 * np.mean([\n",
    "                        (rampToZero(abs(savedData[\"PWitness_median_xp\"]), tolerableAngleOffset, scale = 100e-6) ** 2 if bunchCount == 2 else 0),\n",
    "                        (rampToZero(abs(savedData[\"PWitness_median_yp\"]), tolerableAngleOffset, scale = 100e-6) ** 2 if bunchCount == 2 else 0),\n",
    "                        rampToZero(abs(savedData[\"PDrive_median_xp\"]  ), tolerableAngleOffset, scale = 100e-6) ** 2,\n",
    "                        rampToZero(abs(savedData[\"PDrive_median_yp\"]  ), tolerableAngleOffset, scale = 100e-6) ** 2,\n",
    "            ])\n",
    "        \n",
    "    \n",
    "        #2024-11-25-15-58-15: A desperate, ad hoc move... Trying to prevent optimizer from \"rolling up\" phase space at PENT\n",
    "        #savedData[\"errorTerm_sigma_xp_rule\"] = 1e3 * ( rampToZeroFlip(savedData[f\"PDrive_sigmaSI90_xp\"], 500e-6, 10e-6) ) ** 2\n",
    "    \n",
    "        #2024-11-26-12-38-20: More desperation; force the beam to match design twiss\n",
    "        #savedData[\"errorTerm_BMAG_rule\"] = 1e3 * ( rampToZero(savedData[f\"PDrive_BMAG_x\"], 1.1, 1) + rampToZero(savedData[f\"PDrive_BMAG_y\"], 1.1, 1)) ** 2\n",
    "       \n",
    "        #2024-11-27-10-11-23: Require each slice to comply\n",
    "        # savedData[\"errorTerm_BMAG_rule\"] = enableTransverse * 1e3 * (\n",
    "        #     np.sum([rampToZero(activeBMAG, slicewiseBMAGThreshold, 1) ** 2 for activeBMAG in savedData[f\"PDrive_sliced_BMAG_x\"]]) + \n",
    "        #     np.sum([rampToZero(activeBMAG, slicewiseBMAGThreshold, 1) ** 2 for activeBMAG in savedData[f\"PDrive_sliced_BMAG_y\"]])\n",
    "        # )\n",
    "\n",
    "        #2025-04-04-17-05-10: Special term to prevent optimizer from slipping into pencil beam\n",
    "        #savedData[\"errorTerm_minimalEnergySpread\"] = 1e3 * ( rampToZeroFlip( P[\"sigma_energy\"],  90e6 ) ) ** 2\n",
    "    \n",
    "        \n",
    "        savedData[\"errorTerm_mainObjective\"] = np.mean([\n",
    "\n",
    "                     enableLongitudinalTerms *                   rampToZero(savedData[f\"PDrive_sigmaSI90_z\"],   driveLengthThreshold,   10e-6) ** 2,\n",
    "                    (enableLongitudinalTerms *                   rampToZero(savedData[f\"PWitness_sigmaSI90_z\"], witnessLengthThreshold, 10e-6) ** 2 if bunchCount == 2 else 0),\n",
    "    \n",
    "                    #  enableTransverse *                           rampToZero(savedData[f\"PDrive_sigmaSI90_x\"],   driveSpotThreshold,     10e-6) ** 2,\n",
    "                    #  enableTransverse *                           rampToZero(savedData[f\"PDrive_sigmaSI90_y\"],   driveSpotThreshold,     10e-6) ** 2,\n",
    "                    # (enableTransverse *                           rampToZero(savedData[f\"PWitness_sigmaSI90_x\"], witnessSpotThreshold,   10e-6) ** 2 if bunchCount == 2 else 0),\n",
    "                    # (enableTransverse *                           rampToZero(savedData[f\"PWitness_sigmaSI90_y\"], witnessSpotThreshold,   10e-6) ** 2 if bunchCount == 2 else 0),\n",
    "    \n",
    "                    # enableTransverse *                           rampToZero(savedData[f\"PDrive_emitSI90_x\"],   driveEmittanceThreshold,     10e-6) ** 2,\n",
    "                    # enableTransverse *                           rampToZero(savedData[f\"PDrive_emitSI90_y\"],   driveEmittanceThreshold,     10e-6) ** 2,\n",
    "                    # (enableTransverse *                          rampToZero(savedData[f\"PWitness_emitSI90_x\"], witnessEmittanceThreshold,   10e-6) ** 2 if bunchCount == 2 else 0),\n",
    "                    # (enableTransverse *                          rampToZero(savedData[f\"PWitness_emitSI90_y\"], witnessEmittanceThreshold,   10e-6) ** 2 if bunchCount == 2 else 0),\n",
    "    \n",
    "                    # rampToZero(savedData[f\"PDrive_emitSI90_x\"], 0, 10e-6) ** 2,\n",
    "                    # rampToZero(savedData[f\"PDrive_emitSI90_y\"], 0, 10e-6) ** 2,\n",
    "    \n",
    "                    # rampToZero(savedData[f\"PDrive_norm_emit_x\"], 0, 10e-6) ** 2,\n",
    "                    # rampToZero(savedData[f\"PDrive_norm_emit_y\"], 0, 10e-6) ** 2,\n",
    "    \n",
    "                    #enableTransverse * rampToZero(savedData[f\"PDrive_norm_emit_x\"] * savedData[f\"PDrive_BMAG_x\"], driveEmittanceThreshold, 10e-6) ** 2,\n",
    "                    #enableTransverse * rampToZero(savedData[f\"PDrive_norm_emit_y\"] * savedData[f\"PDrive_BMAG_y\"], driveEmittanceThreshold, 10e-6) ** 2,\n",
    "        ]) \n",
    "    \n",
    "        #Secondary objective includes all \"ramp\" terms with thresholds disabled. Intended to gently nudge all specs to better values if nothing else is going on; mostly expect this to do anything once thresholds are hit\n",
    "        #2024-10-15 comment: Advise not setting this weight above 1e-6. For quite-good mainObjective settings, even 1e-4 is too much\n",
    "        savedData[\"errorTerm_secondaryObjective\"] = 1e-6 * np.mean([\n",
    "                    #(rampToZero( abs(savedData[\"bunchSpacing\"] - targetBunchSpacing), 0 * tolerableBunchSpacingError, scale = 10e-6) ** 2 if bunchCount == 2 else 0),\n",
    "    \n",
    "                    # rampToZero(abs(savedData[\"PDrive_median_x\"]  ),                   0 * tolerableBeamOffset, scale = 1e-6) ** 2,\n",
    "                    # rampToZero(abs(savedData[\"PDrive_median_y\"]  ),                   0 * tolerableBeamOffset, scale = 1e-6) ** 2,\n",
    "                    # (rampToZero(abs(savedData[\"PWitness_median_x\"]),                  0 * tolerableBeamOffset, scale = 1e-6) ** 2 if bunchCount == 2 else 0),\n",
    "                    # (rampToZero(abs(savedData[\"PWitness_median_y\"]),                  0 * tolerableBeamOffset, scale = 1e-6) ** 2 if bunchCount == 2 else 0),\n",
    "    \n",
    "                    # rampToZero(abs(savedData[\"PDrive_median_xp\"]  ),                  0 * tolerableAngleOffset, scale = 100e-6) ** 2,\n",
    "                    # rampToZero(abs(savedData[\"PDrive_median_yp\"]  ),                  0 * tolerableAngleOffset, scale = 100e-6) ** 2,\n",
    "                    # (rampToZero(abs(savedData[\"PWitness_median_xp\"]),                 0 * tolerableAngleOffset, scale = 100e-6) ** 2 if bunchCount == 2 else 0),\n",
    "                    # (rampToZero(abs(savedData[\"PWitness_median_yp\"]),                 0 * tolerableAngleOffset, scale = 100e-6) ** 2 if bunchCount == 2 else 0),\n",
    "    \n",
    "                    rampToZero(savedData[f\"PDrive_sigmaSI90_x\"],                      0 * driveSpotThreshold,     10e-6) ** 2,\n",
    "                    rampToZero(savedData[f\"PDrive_sigmaSI90_y\"],                      0 * driveSpotThreshold,     10e-6) ** 2,\n",
    "                    rampToZero(savedData[f\"PDrive_sigmaSI90_z\"],                      0 * driveLengthThreshold,   10e-6) ** 2,\n",
    "                    (rampToZero(savedData[f\"PWitness_sigmaSI90_x\"],                   0 * witnessSpotThreshold,   10e-6) ** 2 if bunchCount == 2 else 0),\n",
    "                    (rampToZero(savedData[f\"PWitness_sigmaSI90_y\"],                   0 * witnessSpotThreshold,   10e-6) ** 2 if bunchCount == 2 else 0),\n",
    "                    (rampToZero(savedData[f\"PWitness_sigmaSI90_z\"],                   0 * witnessLengthThreshold, 10e-6) ** 2 if bunchCount == 2 else 0),\n",
    "        ])\n",
    "        \n",
    "        savedData[\"maximizeMe\"] = 1 / np.mean([\n",
    "            #Constraints\n",
    "            savedData[\"errorTerm_lostChargeFraction\"], \n",
    "            savedData[\"errorTerm_bunchSpacing\"],\n",
    "            savedData[\"errorTerm_transverseOffset\"],\n",
    "            savedData[\"errorTerm_angleOffset\"],\n",
    "    \n",
    "            #Objectives\n",
    "            savedData[\"errorTerm_mainObjective\"],\n",
    "            savedData[\"errorTerm_secondaryObjective\"],\n",
    "    \n",
    "            #Additional, specialized terms\n",
    "            #savedData[\"errorTerm_sigma_xp_rule\"],\n",
    "            (savedData[\"errorTerm_BMAG_rule\"] if \"errorTerm_BMAG_rule\" in savedData else 0),\n",
    "            (savedData[\"errorTerm_minimalEnergySpread\"] if \"errorTerm_minimalEnergySpread\" in savedData else 0),\n",
    "            \n",
    "            1e-20 #Avoid infinities \n",
    "        ])\n",
    "    \n",
    "        savedData[\"inputBeamFilePathSuffix\"] = inputBeamFilePathSuffix\n",
    "        savedData[\"csrTF\"] = csrTF\n",
    "\n",
    "    except:\n",
    "        print(f\"specificOptimizer() excepted'd while calculating error terms\")\n",
    "        return 0.9 * badValue\n",
    "    \n",
    "\n",
    "    \n",
    "    #Collect desired data as a pandas Series\n",
    "    tmpData = pd.Series( savedData ) \n",
    "    self.history = pd.concat([self.history, tmpData.to_frame().T])\n",
    "\n",
    "    #Optional: Write to file\n",
    "    #self.history.to_json('optimizerHistory.json', orient='records')\n",
    "    n = self.totalNumEvals\n",
    "    if (n < 100) or (n < 1000 and  n % 10 == 0) or (n % 100 == 0): #This gets expensive to write when n >> 10k\n",
    "        self.history.to_json('optimizerHistory.json', orient='records')\n",
    "    \n",
    "    self.updatePlot()\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    sortedHistory = self.history.sort_values(by='maximizeMe', ascending=False)\n",
    "    bestConfigData = sortedHistory.iloc[0]\n",
    "\n",
    "    #Optionally: Write beams for running best case\n",
    "    if savedData[\"maximizeMe\"] == bestConfigData[\"maximizeMe\"]:\n",
    "        \n",
    "        (getBeamAtElement(tao, \"ENDBC14_2\")).write(\"beams/optimizerRunningBestBeam_ENDBC14.h5\")\n",
    "        (getBeamAtElement(tao, \"BEGBC20\")).write(\"beams/optimizerRunningBestBeam_BEGBC20.h5\")\n",
    "        (getBeamAtElement(tao, \"MFFF\")).write(\"beams/optimizerRunningBestBeam_MFFF.h5\")\n",
    "        (getBeamAtElement(tao, \"PENT\")).write(\"beams/optimizerRunningBestBeam_PENT.h5\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    return savedData[\"maximizeMe\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to create optimizerWrapper based on pbounds\n",
    "def create_optimizer_wrapper(pbounds):\n",
    "    param_names = list(pbounds.keys())\n",
    "    \n",
    "    def optimizerWrapper(self, **kwargs):\n",
    "        params = {name: kwargs.get(name, None) for name in param_names}\n",
    "        if None in params.values():\n",
    "            raise ValueError(\"All parameters must be provided\")\n",
    "        return specificOptimizer(self, **params)\n",
    "    \n",
    "    return optimizerWrapper\n",
    "\n",
    "# Create the optimizerWrapper function\n",
    "optimizerWrapper = create_optimizer_wrapper(pbounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125b726c-ad45-4c40-870a-5d34b3d0d194",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### General optimizer setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7955b78-5c54-4205-80b8-cf73062fa36b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OptimizationProblem:\n",
    "    def __init__(self):\n",
    "        self.history = pd.DataFrame()\n",
    "        self.totalNumEvals = 0\n",
    "        self.plot_display_handle = None\n",
    "        self.evals_display_handle = None\n",
    "\n",
    "\n",
    "    def updatePlot(self):\n",
    "        #This function seems to cause a substantial memory leak.\n",
    "        #This hack below reduces the plotting frequency which makes it tolerable\n",
    "\n",
    "        n = self.totalNumEvals\n",
    "        \n",
    "        if (n < 100) or (n < 1000 and  n % 10 == 0) or (n % 100 == 0): \n",
    "            plt.figure()\n",
    "            plotKey = \"maximizeMe\"\n",
    "            plt.plot(np.arange(len(self.history[plotKey])), self.history[plotKey], '-')\n",
    "            \n",
    "            plt.title('Optimization History')\n",
    "            plt.xlabel('Evaluation #')\n",
    "            plt.ylabel(plotKey)\n",
    "            plt.yscale('log')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            \n",
    "            if self.plot_display_handle is None:\n",
    "                self.plot_display_handle = display(plt.gcf(), display_id=True)\n",
    "            else:\n",
    "                update_display(plt.gcf(), display_id=self.plot_display_handle.display_id)\n",
    "            plt.close()\n",
    "\n",
    "        return\n",
    "\n",
    "    def displayEvals(self):\n",
    "        if self.evals_display_handle is None:\n",
    "            self.evals_display_handle = display(f\"Total Num Evals: {self.totalNumEvals}\", display_id=True)\n",
    "        else:\n",
    "            #Can't use '\\n' in this context. <br> doesn't work either\n",
    "            update_display(f\"Total Num Evals: {self.totalNumEvals}, masterToleranceScaling: {masterToleranceScaling}\", display_id=self.evals_display_handle.display_id)\n",
    "\n",
    "# Attach the function to the class as a method\n",
    "OptimizationProblem.optimizerWrapper = optimizerWrapper\n",
    "\n",
    "# Instantiate the optimization problem\n",
    "problem = OptimizationProblem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636c9a69-ac6f-406c-b7c7-1a0801da07bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "# Convert pbounds to the format required by differential_evolution\n",
    "bounds = [(low, high) for (low, high) in pbounds.values()]\n",
    "param_names = list(pbounds.keys())\n",
    "\n",
    "#scipy.optimize wants to optimize a function which is passed a vector of all the parameters\n",
    "#This programmatically wraps the existing wrapper (ugh...) to handle this format\n",
    "def create_DE_wrapper(obj, param_names):\n",
    "    def wrapperDE(params):\n",
    "        param_dict = dict(zip(param_names, params))\n",
    "\n",
    "        #bayes_opt is a maximizer but differential_evolution is a minimizer... hence the inversion\n",
    "        return -1*obj.optimizerWrapper(**param_dict)\n",
    "    \n",
    "    return wrapperDE\n",
    "\n",
    "wrapperDE = create_DE_wrapper(problem, param_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86c1a72-1d2e-4dd7-9056-c34bfa63a40f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "defaultSettingsVector = [importedDefaultSettings[key] for key in list(pbounds.keys())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477c9433-724f-429d-9ea4-54c010f39ffe",
   "metadata": {},
   "source": [
    "## Run optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43e77d2-b6ce-4963-aff9-239f373446f8",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "if usingCheckpoint:\n",
    "    display(HTML(f'<span style=\"color: red; font-size: 18px;\">Checkpoint enabled: {checkpointElement}</span>'))\n",
    "else:\n",
    "    display(HTML('<span style=\"color: green; font-size: 18px;\">No checkpoint. Start-to-end simulation</span>'))\n",
    "\n",
    "if masterToleranceScalingStart == masterToleranceScalingEnd == 1.0:\n",
    "    display(HTML('<span style=\"color: green; font-size: 18px;\">No masterToleranceScaling</span>'))\n",
    "else:\n",
    "    display(HTML('<span style=\"color: red; font-size: 18px;\">masterToleranceScaling is changing</span>'))\n",
    "\n",
    "if csrTF:\n",
    "    display(HTML('<span style=\"color: green; font-size: 18px;\">CSR enabled</span>'))\n",
    "else:\n",
    "    display(HTML('<span style=\"color: red; font-size: 18px;\">CSR disabled</span>'))\n",
    "\n",
    "if evalElement == \"PENT\":\n",
    "    display(HTML('<span style=\"color: green; font-size: 18px;\">Evaluation at PENT</span>'))\n",
    "else:\n",
    "    display(HTML(f'<span style=\"color: red; font-size: 18px;\">Evaluation at {evalElement}, not PENT</span>'))\n",
    "\n",
    "if stageOneOptimization:\n",
    "    display(HTML('<span style=\"color: red; font-size: 18px;\">Stage 1 optimization disables spot size, angle, and offset penalties </span>'))\n",
    "\n",
    "if not enableAlignmentTerms:\n",
    "    display(HTML('<span style=\"color: red; font-size: 18px;\">Offset and angle penalties disabled</span>'))\n",
    "\n",
    "    if importedDefaultSettings[\"centerMFFF\"]:\n",
    "        display(HTML('<span style=\"color: orange; font-size: 18px;\">---However, centerMFFF is on</span>'))\n",
    "\n",
    "if not enableLongitudinalTerms:\n",
    "    display(HTML('<span style=\"color: red; font-size: 18px;\">Bunch spacing and length terms disabled</span>'))\n",
    "\n",
    "if abs(PInit.charge - 1.6e-9) > 0.05e-9:\n",
    "    display(HTML(f'<span style=\"color: red; font-size: 18px;\">Atypical charge. {PInit.charge * 1e9} nC not 1.6 nC</span>'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294e9bdb-4ae7-46c9-b0fd-9f729563b7da",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "optimizationType = \"differentialEvolution\" #Classic, standard for exploration\n",
    "#optimizationType = \"nelderMead\"            #Classic, standard for refinement\n",
    "#optimizationType = \"L-BFGS-B\"              #This never seems to work; I think the problem is too noisy to get a reasonable Hessian\n",
    "#optimizationType = \"powell\"                #This rarely seems to help; I think the orthogonal approach is not appropriate for this problem\n",
    "#optimizationType = \"COBYQA\"                #Seems promising. Not timid about making big, coordinated moves. I like it more for refinement though. Seems (much) more likely than nelderMead to give up too early though...\n",
    "#optimizationType = \"bayesian\"              #This uses a different library, different signature, different objective, different bounds, etc... beware!\n",
    "\n",
    "match optimizationType:\n",
    "\n",
    "    case \"differentialEvolution\":\n",
    "        totalPopSize = 50            # Total population size\n",
    "        numDimensions = len(bounds)  # Number of parameters\n",
    "        \n",
    "        #initializationType = \"uniform\"\n",
    "        initializationType = \"normal\"\n",
    "        \n",
    "        match initializationType:\n",
    "        \n",
    "            case \"uniform\":\n",
    "                populationDE = np.random.rand(totalPopSize, numDimensions)\n",
    "                for i in range(numDimensions):\n",
    "                    low, high = bounds[i]\n",
    "                    populationDE[:, i] = low + populationDE[:, i] * (high - low)  \n",
    "                    \n",
    "            case \"normal\":\n",
    "                populationDE = np.zeros((totalPopSize, numDimensions))\n",
    "                for i in range(numDimensions):\n",
    "                    low, high = bounds[i]\n",
    "                \n",
    "                    #Define mean based on bounds\n",
    "                    #mean = (high + low) / 2\n",
    "                \n",
    "                    #Optional: Define mean based on defaultSettingsVector\n",
    "                    mean = defaultSettingsVector[i]\n",
    "                    \n",
    "                    std_dev = (high - low) / 10\n",
    "                    populationDE[:, i] = np.random.normal(mean, std_dev, totalPopSize)\n",
    "\n",
    "        #Optional: Add specific points to initial evaluation list\n",
    "        populationDE = np.vstack([[ defaultSettingsVector ], populationDE])\n",
    "        \n",
    "        result = differential_evolution(\n",
    "            wrapperDE, \n",
    "            bounds,\n",
    "            maxiter=500, \n",
    "            disp=True,\n",
    "            polish = False, \n",
    "            init = populationDE\n",
    "        )\n",
    "\n",
    "    case \"nelderMead\":\n",
    "        #Optional: Manually define starting simplex. Need to enable in minimize() options\n",
    "        numDimensions = len(defaultSettingsVector)\n",
    "        initial_simplex = np.tile(defaultSettingsVector, (numDimensions + 1, 1))\n",
    "        for i in range(1, numDimensions + 1):\n",
    "            initial_simplex[i][i - 1] += 1e-2 * (bounds[i-1][1] - bounds[i-1][0])\n",
    "        \n",
    "        result = minimize(\n",
    "            wrapperDE, \n",
    "            defaultSettingsVector,\n",
    "            method = \"Nelder-Mead\",\n",
    "            bounds = bounds,\n",
    "            options={'initial_simplex': initial_simplex, 'adaptive': True},\n",
    "            #options={'adaptive': True}\n",
    "        )\n",
    "\n",
    "    case \"L-BFGS-B\":\n",
    "        result = minimize(\n",
    "            wrapperDE, \n",
    "            defaultSettingsVector,\n",
    "            method = \"L-BFGS-B\",\n",
    "            bounds = bounds,\n",
    "            #options = {\"eps\": [1e-4*x for x in defaultSettingsVector]}\n",
    "            options = {\"eps\": [1e-3*(x[1]-x[0]) for x in pbounds.values()]}\n",
    "        )\n",
    "\n",
    "\n",
    "    case \"powell\":\n",
    "        result = minimize(\n",
    "            wrapperDE, \n",
    "            defaultSettingsVector,\n",
    "            method = \"powell\",\n",
    "            bounds = bounds,\n",
    "        )\n",
    "\n",
    "    case \"COBYQA\":\n",
    "        result = minimize(\n",
    "            wrapperDE, \n",
    "            defaultSettingsVector,\n",
    "            method = \"COBYQA\",\n",
    "            bounds = bounds,\n",
    "            options = {\n",
    "                \"scale\": True, \n",
    "                \"initial_tr_radius\": 1e-1, #1e-1\n",
    "                \"final_tr_radius\": 1e-8, \n",
    "                \"disp\": True } #\"*_tr_radius\" appears to constrain the scaled, not raw, variables (if scaling is enabled)\n",
    "        )\n",
    "\n",
    "    case \"bayesian\":\n",
    "        optimizer = bayes_opt.BayesianOptimization(\n",
    "                f=problem.optimizerWrapper,\n",
    "                pbounds=pbounds,\n",
    "                random_state=1,\n",
    "                allow_duplicate_points=True, #2024-04-26 it was whining about this\n",
    "                verbose = 0\n",
    "        )\n",
    "        \n",
    "        #Initial point(s) to check\n",
    "        optimizer.probe(\n",
    "            params={key: importedDefaultSettings[key] for key in pbounds.keys()},\n",
    "            lazy=True,\n",
    "        )\n",
    "        \n",
    "        #Refer to https://bayesian-optimization.github.io/BayesianOptimization/exploitation_vs_exploration.html\n",
    "        #and https://github.com/bayesian-optimization/BayesianOptimization/blob/master/bayes_opt/bayesian_optimization.py\n",
    "        boInitialKappa = 10 + 0*10.\n",
    "        boFinalKappa = 1e-6 + 0*0.1\n",
    "        boNumIter = 2000\n",
    "        boKappaDecay = (boFinalKappa / boInitialKappa)**(1/boNumIter)\n",
    "        \n",
    "        acquisition_function = bayes_opt.util.UtilityFunction(\n",
    "                                           kind='ucb',\n",
    "                                           kappa=boInitialKappa,         #Default 2.576\n",
    "                                           xi=0.0,              #Default 0\n",
    "                                           kappa_decay=boKappaDecay,       #Default 0\n",
    "                                           kappa_decay_delay=0  #Default 0                     \n",
    "                                        )\n",
    "        \n",
    "        optimizer.maximize(\n",
    "            init_points=200, #Initial, random points. Unwise to omit. \"Often, 2*dim to 5*dim random points are recommended before switching to model-driven exploration.\"\n",
    "            n_iter=boNumIter,\n",
    "            acquisition_function=acquisition_function\n",
    "        )\n",
    "\n",
    "print(problem.history) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537d5d94-ee16-4a6d-af06-53139a015f2b",
   "metadata": {},
   "source": [
    "## Check out results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50f42b1-5f6b-4a17-99ac-3902f40189bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#problem.history.to_csv('output_data.csv', index=False)\n",
    "\n",
    "problem.history = problem.history.sort_values(by='maximizeMe', ascending=False)\n",
    "\n",
    "\n",
    "bestConfigData = problem.history.iloc[0]\n",
    "bestConfigDict = bestConfigData.to_dict()\n",
    "\n",
    "print( bestConfigData ) \n",
    "\n",
    "\n",
    "\n",
    "setLattice(tao, **bestConfigDict)\n",
    "\n",
    "\n",
    "trackBeam(tao, **importedDefaultSettings, verbose = True)\n",
    "\n",
    "P = getBeamAtElement(tao, \"PENT\")\n",
    "PDrive, PWitness = getDriverAndWitness(P)\n",
    "\n",
    "print(f\"\"\"P, sigma x: {P[\"sigma_x\"]}\"\"\")\n",
    "print(f\"\"\"PDrive, sigma x: {PDrive[\"sigma_x\"]}\"\"\")\n",
    "print(f\"\"\"PWitness, sigma x: {PWitness[\"sigma_x\"]}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dec3b8-84cd-4210-955d-3ff84b7e60b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bestConfigDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041feb20-cbd4-4875-a53a-27e1673b6381",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(plotMod(P, 'x', 'y',  bins=300))\n",
    "display(plotMod(P, 'x', 'pz', bins=300))\n",
    "display(plotMod(P, 'x', 'xp', bins=300))\n",
    "display(plotMod(P, 'y', 'yp', bins=300))\n",
    "display(plotMod(P, 'delta_t', 'pz', bins=300))\n",
    "display(slicePlotMod(P, 'norm_emit_x',n_slice=100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e145e5-5de1-4039-8a0e-7615383bd0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(plotMod(PWitness, 'x', 'y',  bins=300))\n",
    "display(plotMod(PWitness, 'x', 'pz', bins=300))\n",
    "display(plotMod(PWitness, 'x', 'xp', bins=300))\n",
    "display(plotMod(PWitness, 'y', 'yp', bins=300))\n",
    "display(plotMod(PWitness, 'delta_t', 'pz', bins=300))\n",
    "display(slicePlotMod(PWitness, 'norm_emit_x',n_slice=100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e873d9e9-c62f-40e3-859e-798648f88cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"P, sigma x: {P[\"sigma_x\"]}\"\"\")\n",
    "print(f\"\"\"PDrive, sigma x: {PDrive[\"sigma_x\"]}\"\"\")\n",
    "print(f\"\"\"PWitness, sigma x: {PWitness[\"sigma_x\"]}\"\"\")\n",
    "\n",
    "display(plotMod(P, 'x', 'y',  bins=300))\n",
    "display(plotMod(PDrive, 'x', 'y',  bins=300))\n",
    "display(plotMod(PWitness, 'x', 'y',  bins=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9d5824-6cf6-4e11-8aae-00d366d69c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = getBeamAtElement(tao, \"MFFF\")\n",
    "\n",
    "display(plotMod(P, 'x', 'xp', bins=300))\n",
    "display(slicePlotMod(P, 'norm_emit_x',n_slice=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7c2188-44ab-4d6b-8d52-bbb3c47f758d",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368fee55-6580-4f5e-a583-3891914f517b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
