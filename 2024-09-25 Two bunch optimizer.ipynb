{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f79f63b1-f554-4489-a552-104b6617fc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from UTILITY_quickstart import *\n",
    "\n",
    "with open('setLattice_defaults.yml', 'r') as file:\n",
    "    importedDefaultSettings = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45a9d06a-d692-444b-8e0b-9931f1aa3934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment set to:  /Users/nmajik/Documents/SLAC/FACET2-Bmad-PyTao\n",
      "Tracking to end\n",
      "CSR off\n",
      "Overwriting lattice with setLattice() defaults\n",
      "No defaults file provided to setLattice(). Using setLattice_defaults.yml\n",
      "Number of macro particles = 10000.0\n",
      "Loaded activeBeamFile.h5\n",
      "Set track_start = L0AFEND, track_end = L0BFEND\n",
      "Tracking!\n",
      "trackBeam() exiting\n"
     ]
    }
   ],
   "source": [
    "tao = initializeTao(\n",
    "    inputBeamFilePathSuffix = '/beams/nmmToL0AFEND_2bunch_2024-02-16Clean/2024-02-16_2bunch_1e5Downsample_nudgeWeights.h5',\n",
    "\n",
    "    csrTF = False,\n",
    "    numMacroParticles=1e4,\n",
    ")\n",
    "\n",
    "#Set aside the initial beam for later reference\n",
    "trackBeam(tao, trackEnd = \"L0BFEND\", verbose = True)\n",
    "PInit = ParticleGroup(data=tao.bunch_data(\"L0AFEND\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81e60ee4-cc1c-471f-8450-3ed53e56e0af",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (2816623055.py, line 47)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 47\u001b[0;36m\u001b[0m\n\u001b[0;31m    'L0BEnergyOffset': (-5e6, 5e6)\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "targetBunchSpacing = 150e-6\n",
    "\n",
    "#masterToleranceScaling = 1.0 #Higher is looser; generally tighten for early sims and loosen for refinement\n",
    "masterToleranceScalingStart = 1.0\n",
    "masterToleranceScalingEnd = 1.0\n",
    "masterToleranceScalingEvolutionSteps = 200\n",
    "masterToleranceScaling = masterToleranceScalingStart\n",
    "\n",
    "usingCheckpoint = False\n",
    "#checkpointElement = \"MFFF\"\n",
    "#checkpointElement = \"IM1988\" #Shortly upstream of BEGBC20\n",
    "checkpointElement = \"CB1LE\" #Shortly downstream of BEGBC20; there's an intervening dipole though\n",
    "checkpointElementS = tao.ele_param(checkpointElement,\"ele.s\")['ele_s']\n",
    "centerBC14 = True \n",
    "centerBC20 = True\n",
    "\n",
    "#If the checkpoint is DOWNSTREAM of the centering location AND you want centering, set True\n",
    "centerBC14ForCheckpoint = True and (checkpointElementS > tao.ele_param(\"BEGBC14_1\",\"ele.s\")['ele_s'])\n",
    "centerBC20ForCheckpoint = True and (checkpointElementS > tao.ele_param(\"BEGBC20\",\"ele.s\")['ele_s'])\n",
    "\n",
    "#If the checkpoint is UPSTREAM of the centering location AND you want centering, set True\n",
    "centerBC14ForRemainder = centerBC14 and not centerBC14ForCheckpoint\n",
    "centerBC20ForRemainder = centerBC20 and not centerBC20ForCheckpoint\n",
    "\n",
    "if usingCheckpoint:\n",
    "    print(f\"Using checkpoint: {checkpointElement}\")\n",
    "    print(f\"centerBC14ForCheckpoint = {centerBC14ForCheckpoint}, centerBC20ForCheckpoint = {centerBC20ForCheckpoint}\")\n",
    "    print(f\"centerBC14ForRemainder = {centerBC14ForRemainder}, centerBC20ForRemainder = {centerBC20ForRemainder}\")\n",
    "else:\n",
    "    print(\"No checkpoint. Start to end tracking\")\n",
    "    print(f\"centerBC14 = {centerBC14}, centerBC20 = {centerBC20}\")\n",
    "\n",
    "\n",
    "pbounds = {\n",
    "    # \"QA10361kG\": eval(importedDefaultSettings[\"QA10361kGBounds\"]),\n",
    "    # \"QA10371kG\": eval(importedDefaultSettings[\"QA10371kGBounds\"]),\n",
    "    # \"QE10425kG\": eval(importedDefaultSettings[\"QE10425kGBounds\"]),\n",
    "    # \"QE10441kG\": eval(importedDefaultSettings[\"QE10441kGBounds\"]),\n",
    "    # \"QE10511kG\": eval(importedDefaultSettings[\"QE10511kGBounds\"]),\n",
    "    # \"QE10525kG\": eval(importedDefaultSettings[\"QE10525kGBounds\"]),\n",
    "\n",
    "    'L0BPhaseSet': (-15, 15),\n",
    "    'L1PhaseSet': (-60, 0),\n",
    "    'L2PhaseSet': (-60, 0),\n",
    "    'L3PhaseSet': (-15, 15),\n",
    "    \n",
    "    'L0BEnergyOffset': (-5e6, 5e6),\n",
    "    'L1EnergyOffset': (-10e6, 10e6),\n",
    "    'L2EnergyOffset': (-500e6, 500e6),\n",
    "    'L3EnergyOffset': (-500e6, 500e6),\n",
    "\n",
    "    # \"Q1EkG\":  eval(importedDefaultSettings[\"Q1EkGBounds\"]),\n",
    "    # \"Q2EkG\":  eval(importedDefaultSettings[\"Q2EkGBounds\"]),\n",
    "    # \"Q3EkG\":  eval(importedDefaultSettings[\"Q3EkGBounds\"]),\n",
    "    # \"Q4EkG\":  eval(importedDefaultSettings[\"Q4EkGBounds\"]),\n",
    "    # \"Q5EkG\":  eval(importedDefaultSettings[\"Q5EkGBounds\"]),\n",
    "    # \"Q6EkG\":  eval(importedDefaultSettings[\"Q6EkGBounds\"]),\n",
    "    \n",
    "    # \"S1ELkG\": eval(importedDefaultSettings[\"S1ELkGBounds\"]),\n",
    "    # \"S2ELkG\": eval(importedDefaultSettings[\"S2ELkGBounds\"]),\n",
    "    # \"S3ELkG\": eval(importedDefaultSettings[\"S3ELkGBounds\"]),\n",
    "    # \"S3ERkG\": eval(importedDefaultSettings[\"S3ERkGBounds\"]),\n",
    "    # \"S2ERkG\": eval(importedDefaultSettings[\"S2ERkGBounds\"]),\n",
    "    # \"S1ERkG\": eval(importedDefaultSettings[\"S1ERkGBounds\"]),\n",
    "\n",
    "    \"S1EL_xOffset\" : ( -0.004, 0.004 ),  \n",
    "    \"S1EL_yOffset\" : ( -0.004, 0.004 ),  \n",
    "    \"S2EL_xOffset\" : ( -0.004, 0.004 ),  \n",
    "    \"S2EL_yOffset\" : ( -0.004, 0.004 ),  \n",
    "    \"S2ER_xOffset\" : ( -0.004, 0.004 ),  \n",
    "    \"S2ER_yOffset\" : ( -0.004, 0.004 ),  \n",
    "    \"S1ER_xOffset\" : ( -0.004, 0.004 ),  \n",
    "    \"S1ER_yOffset\" : ( -0.004, 0.004 ),\n",
    "\n",
    "    # 'Q5FFkG': eval(importedDefaultSettings[\"Q5FFkGBounds\"]),\n",
    "    # 'Q4FFkG': eval(importedDefaultSettings[\"Q4FFkGBounds\"]),\n",
    "    # 'Q3FFkG': eval(importedDefaultSettings[\"Q3FFkGBounds\"]),\n",
    "    # 'Q2FFkG': eval(importedDefaultSettings[\"Q2FFkGBounds\"]),\n",
    "    # 'Q1FFkG': eval(importedDefaultSettings[\"Q1FFkGBounds\"]),\n",
    "    # 'Q0FFkG': eval(importedDefaultSettings[\"Q0FFkGBounds\"]),\n",
    "    # 'Q0DkG':  eval(importedDefaultSettings[\"Q0DkGBounds\"]),\n",
    "    # 'Q1DkG':  eval(importedDefaultSettings[\"Q1DkGBounds\"]),\n",
    "    # 'Q2DkG':  eval(importedDefaultSettings[\"Q2DkGBounds\"]),\n",
    "\n",
    "    \"XC1FFkG\" : eval(importedDefaultSettings[\"XC1FFkGBounds\"]),\n",
    "    \"XC3FFkG\" : eval(importedDefaultSettings[\"XC3FFkGBounds\"]),\n",
    "    \"YC1FFkG\" : eval(importedDefaultSettings[\"YC1FFkGBounds\"]),\n",
    "    \"YC2FFkG\" : eval(importedDefaultSettings[\"YC2FFkGBounds\"]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c8c357-a2a7-4e99-bdb1-4e53ed6e5393",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704ad00f-d1d2-42f6-a734-50be8ad97a45",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ed0772-8ef8-4cdb-a890-83a3d7c5fd1a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #2024-08-23: Now to IM1988 to enable centering at BC20\n",
    "# def propagateToBEGBC20(): \n",
    "\n",
    "#     ##################################\n",
    "#     #Propagate to BEGBC20 and save result\n",
    "#     ##################################\n",
    "#     makeBeamActiveBeamFile(PInit)\n",
    "    \n",
    "\n",
    "#     trackBeam(tao, trackStart = \"L0AFEND\", trackEnd = \"IM1988\", centerBC14 = True, verbose = True)\n",
    "\n",
    "#     P = ParticleGroup(data=tao.bunch_data(\"IM1988\"))\n",
    "    \n",
    "#     makeBeamActiveBeamFile(P)\n",
    "\n",
    "#     return\n",
    "\n",
    "\n",
    "#Generalizing\n",
    "def propagateFromStartToCheckpoint(\n",
    "    checkpointElement,\n",
    "    centerBC14,\n",
    "    centerBC20\n",
    "): \n",
    "    makeBeamActiveBeamFile(PInit)\n",
    "\n",
    "    trackBeam(tao, trackStart = \"L0AFEND\", trackEnd = checkpointElement, centerBC14 = centerBC14, centerBC20 = centerBC20, verbose = False)\n",
    "\n",
    "    P = ParticleGroup(data=tao.bunch_data(checkpointElement))\n",
    "    \n",
    "    makeBeamActiveBeamFile(P)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1e0d14-7eb3-4bcc-ba7d-0c71a5a83895",
   "metadata": {},
   "source": [
    "## Optimizer functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e8cd7a-9adb-4abd-a60d-010d2ca631a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if usingCheckpoint:\n",
    "    setLattice(tao) #Set lattice to current default config\n",
    "    propagateFromStartToCheckpoint(checkpointElement, centerBC14ForCheckpoint, centerBC20ForCheckpoint)\n",
    "\n",
    "def rampToZero(val, thresh, scale = 1):\n",
    "    return (max(val, thresh) - thresh) / scale\n",
    "\n",
    "\n",
    "def updateMasterToleranceScaling(totalNumEvals):\n",
    "    \"\"\"\n",
    "    I'm not sure if this is actually a good idea or not. \n",
    "    The general idea I'm aiming for is that early optimization iterations will get closer to the thresholded constraints than strictly necessary, \n",
    "    allowing later optimization to proceed with more wiggle room and less likelihood of falling off a \"cliff\".\n",
    "    Maybe it'd make more sense to make this conditional on something other than evaluation count?\n",
    "    Alternatively, what if it's useful to start with looser constraints and gradually tighten them? For something like COBYQA, it's less likely to \"overlearn\" when trying to satisfy, e.g. the first condition\n",
    "    An insane person might try using an exponentially decaying sine to try both...\n",
    "    \"\"\"\n",
    "    \n",
    "    global masterToleranceScaling\n",
    "\n",
    "    if totalNumEvals < masterToleranceScalingEvolutionSteps: \n",
    "        masterToleranceScaling = masterToleranceScalingStart + (masterToleranceScalingEnd - masterToleranceScalingStart) * totalNumEvals / masterToleranceScalingEvolutionSteps\n",
    "    else:\n",
    "        masterToleranceScaling = masterToleranceScalingEnd\n",
    "    \n",
    "    return\n",
    "\n",
    "def specificOptimizer(\n",
    "    self,\n",
    "    **kwargs\n",
    "):\n",
    "\n",
    "    self.totalNumEvals += 1\n",
    "    self.displayEvals()\n",
    "\n",
    "    updateMasterToleranceScaling(self.totalNumEvals)\n",
    "\n",
    "    savedData = kwargs\n",
    "    \n",
    "    badValue = -1e30  #The value returned for illegal config. Should be colossal. Double limit ~= 1e308\n",
    "    bigCost  = 1e20   #Should be large enough to dominate any \"normal\" return value but be dominated by badValue\n",
    "    \n",
    "    try: #This try block deals with bad configurations. Instead of causing the optimizer to halt we now 'except' a low value\n",
    "        setLattice(tao, **kwargs)\n",
    "\n",
    "    except:\n",
    "        print(f\"specificOptimizer() excepted'd on setLattice()\")\n",
    "        return badValue * 5\n",
    "\n",
    "    try:\n",
    "        if usingCheckpoint: \n",
    "            trackBeam(tao, trackStart = checkpointElement, trackEnd = \"end\", centerBC14 = centerBC14ForRemainder, centerBC20 = centerBC20ForRemainder, verbose = False)\n",
    "        else:\n",
    "            trackBeam(tao, trackStart = \"L0AFEND\", trackEnd = \"end\", centerBC14 = centerBC14, centerBC20 = centerBC20, verbose = False)\n",
    "\n",
    "    except:\n",
    "            print(f\"specificOptimizer() excepted'd on trackBeam()\")\n",
    "            return badValue * 4\n",
    "    \n",
    "\n",
    "    #BEGBC20NumLiveParticles = tao.bunch_params(\"BEGBC20\")['n_particle_live']\n",
    "    #PENTNumLiveParticles = tao.bunch_params(\"PENT\")['n_particle_live']\n",
    "\n",
    "    if tao.bunch_params(\"end\")['n_particle_live'] < 10:\n",
    "        print(f\"specificOptimizer() got ~no particles after tracking\")\n",
    "        return badValue * 2 \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    try: \n",
    "        P = getBeamAtElement(tao, \"PENT\")\n",
    "        PDrive, PWitness = getDriverAndWitness(P)\n",
    "    \n",
    "\n",
    "        for PActiveStr in [\"PDrive\", \"PWitness\"]:\n",
    "            PActive = locals()[PActiveStr]\n",
    "            for val in [\"mean_x\", \"mean_y\", \"sigma_x\", \"sigma_y\", \"mean_xp\", \"mean_yp\"]:\n",
    "                savedData[f\"{PActiveStr}_{val}\"] = PActive[val]\n",
    "                #print(f\"\"\"{PActiveStr}_{val} = {PActive[val]}\"\"\")\n",
    "\n",
    "\n",
    "            savedData[f\"{PActiveStr}_median_x\"] = np.median(PActive.x)\n",
    "            savedData[f\"{PActiveStr}_median_y\"] = np.median(PActive.y)\n",
    "\n",
    "            savedData[f\"{PActiveStr}_median_xp\"] = np.median(PActive.xp)\n",
    "            savedData[f\"{PActiveStr}_median_yp\"] = np.median(PActive.yp)\n",
    "            \n",
    "            savedData[f\"{PActiveStr}_sigmaSI90_x\"] = smallestIntervalImpliedSigma(PActive.x, percentage = 0.90)\n",
    "            savedData[f\"{PActiveStr}_sigmaSI90_y\"] = smallestIntervalImpliedSigma(PActive.y, percentage = 0.90)\n",
    "            savedData[f\"{PActiveStr}_sigmaSI90_z\"] = smallestIntervalImpliedSigma(PActive.t * 3e8, percentage=0.9)\n",
    "\n",
    "            savedData[f\"{PActiveStr}_emitSI90_x\"] = smallestIntervalImpliedEmittance(PActive, plane = \"x\", percentage = 0.90)\n",
    "            savedData[f\"{PActiveStr}_emitSI90_y\"] = smallestIntervalImpliedEmittance(PActive, plane = \"y\", percentage = 0.90)\n",
    "\n",
    "            #savedData[f\"{PActiveStr}_zLen\"] = smallestIntervalImpliedSigma(PActive.t * 3e8, percentage=0.9)\n",
    "\n",
    "            savedData[f\"{PActiveStr}_zCentroid\"] = np.median(PActive.t * 3e8)\n",
    "\n",
    "        savedData[\"bunchSpacing\"] = savedData[\"PWitness_zCentroid\"] - savedData[\"PDrive_zCentroid\"]\n",
    "\n",
    "        savedData[\"transverseCentroidOffset\"] = np.sqrt(\n",
    "                (savedData[\"PDrive_mean_x\"] - savedData[\"PWitness_mean_x\"])**2 + \n",
    "                (savedData[\"PDrive_mean_y\"] - savedData[\"PWitness_mean_y\"])**2\n",
    "            )\n",
    "\n",
    "        \n",
    "        #savedData[\"lostChargeFraction\"] = 1 - (tao.bunch_params(\"DTOTR\")['n_particle_live'] / tao.bunch_params(\"BEGBC20\")['n_particle_live'])\n",
    "        savedData[\"lostChargeFraction\"] = 1 - (P.charge / PInit.charge)\n",
    "            \n",
    "\n",
    "    except:\n",
    "        print(f\"specificOptimizer() excepted'd while getting beam and compiling savedData\")\n",
    "        return badValue\n",
    "\n",
    "\n",
    "    #print(f\"The MTS I see is {masterToleranceScaling}\")\n",
    "    tolerableBeamLossFraction = 0.02 * masterToleranceScaling\n",
    "    tolerableBunchSpacingError = 10e-6 * masterToleranceScaling\n",
    "    \n",
    "    tolerableBeamOffset  = 5e-6 * masterToleranceScaling\n",
    "    tolerableAngleOffset = 5e-3 * masterToleranceScaling\n",
    "    \n",
    "\n",
    "    driveEmittanceThreshold   = 50e-6 * masterToleranceScaling\n",
    "    witnessEmittanceThreshold = 20e-6 * masterToleranceScaling\n",
    "\n",
    "    driveSpotThreshold   = 30e-6 * masterToleranceScaling\n",
    "    witnessSpotThreshold = 30e-6 * masterToleranceScaling\n",
    "    \n",
    "    driveLengthThreshold   = 20e-6 * masterToleranceScaling\n",
    "    witnessLengthThreshold = 20e-6 * masterToleranceScaling\n",
    "\n",
    "    \n",
    "    savedData[\"errorTerm_lostChargeFraction\"] = 1e3 * rampToZero( savedData[\"lostChargeFraction\"], tolerableBeamLossFraction, scale = 0.01)**2\n",
    "    \n",
    "    savedData[\"errorTerm_bunchSpacing\"] = 1e3 * rampToZero( abs(savedData[\"bunchSpacing\"] - targetBunchSpacing), tolerableBunchSpacingError, scale = 10e-6)**2\n",
    "    \n",
    "    savedData[\"errorTerm_transverseOffset\"] = 1e3 * np.mean([\n",
    "                rampToZero(abs(savedData[\"PWitness_median_x\"]), tolerableBeamOffset, scale = 1e-6)**2,\n",
    "                rampToZero(abs(savedData[\"PWitness_median_y\"]), tolerableBeamOffset, scale = 1e-6)**2,\n",
    "                rampToZero(abs(savedData[\"PDrive_median_x\"]  ), tolerableBeamOffset, scale = 1e-6)**2,\n",
    "                rampToZero(abs(savedData[\"PDrive_median_y\"]  ), tolerableBeamOffset, scale = 1e-6)**2,\n",
    "    ])\n",
    "    \n",
    "    savedData[\"errorTerm_angleOffset\"] = 1e3 * np.mean([\n",
    "                rampToZero(abs(savedData[\"PWitness_median_xp\"]), tolerableAngleOffset, scale = 100e-6)**2,\n",
    "                rampToZero(abs(savedData[\"PWitness_median_yp\"]), tolerableAngleOffset, scale = 100e-6)**2,\n",
    "                rampToZero(abs(savedData[\"PDrive_median_xp\"]  ), tolerableAngleOffset, scale = 100e-6)**2,\n",
    "                rampToZero(abs(savedData[\"PDrive_median_yp\"]  ), tolerableAngleOffset, scale = 100e-6)**2,\n",
    "    ])\n",
    "    \n",
    "    # savedData[\"errorTerm_mainObjective\"] = max([\n",
    "    #             savedData[f\"PDrive_sigmaSI90_x\"],savedData[f\"PDrive_sigmaSI90_y\"],savedData[f\"PDrive_zLen\"],\n",
    "    #             savedData[f\"PWitness_sigmaSI90_x\"],savedData[f\"PWitness_sigmaSI90_y\"],savedData[f\"PWitness_zLen\"]\n",
    "    # ]) / 10e-6\n",
    "\n",
    "    savedData[\"errorTerm_mainObjective\"] = np.mean([\n",
    "                rampToZero(savedData[f\"PDrive_sigmaSI90_x\"], driveSpotThreshold, 10e-6),\n",
    "                rampToZero(savedData[f\"PDrive_sigmaSI90_y\"], driveSpotThreshold, 10e-6),\n",
    "                rampToZero(savedData[f\"PDrive_sigmaSI90_z\"], driveLengthThreshold, 10e-6),\n",
    "\n",
    "                rampToZero(savedData[f\"PWitness_sigmaSI90_x\"], witnessSpotThreshold, 10e-6),\n",
    "                rampToZero(savedData[f\"PWitness_sigmaSI90_y\"], witnessSpotThreshold, 10e-6),\n",
    "                rampToZero(savedData[f\"PWitness_sigmaSI90_z\"], witnessLengthThreshold, 10e-6),\n",
    "    ]) \n",
    "    \n",
    "    # savedData[\"errorTerm_secondaryObjective\"] = 0.1 * np.mean([\n",
    "    #             savedData[f\"PDrive_sigmaSI90_x\"],savedData[f\"PDrive_sigmaSI90_y\"],savedData[f\"PDrive_zLen\"],\n",
    "    #             savedData[f\"PWitness_sigmaSI90_x\"],savedData[f\"PWitness_sigmaSI90_y\"],savedData[f\"PWitness_zLen\"]\n",
    "    # ]) / 10e-6\n",
    "    \n",
    "    savedData[\"maximizeMe\"] = 1 / np.mean([\n",
    "        savedData[\"errorTerm_lostChargeFraction\"], \n",
    "        savedData[\"errorTerm_bunchSpacing\"],\n",
    "        savedData[\"errorTerm_transverseOffset\"],\n",
    "        savedData[\"errorTerm_angleOffset\"],\n",
    "        savedData[\"errorTerm_mainObjective\"],\n",
    "        #savedData[\"errorTerm_secondaryObjective\"]\n",
    "        1e-20 #Avoid infinities \n",
    "    ])\n",
    "\n",
    "  \n",
    "    \n",
    "    #Collect desired data as a pandas Series\n",
    "    tmpData = pd.Series( savedData ) \n",
    "    self.history = pd.concat([self.history, tmpData.to_frame().T])\n",
    "\n",
    "    #Optional: Write to file\n",
    "    self.history.to_json('optimizerHistory.json', orient='records')\n",
    "    \n",
    "    self.updatePlot()\n",
    "\n",
    "\n",
    "    return savedData[\"maximizeMe\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to create optimizerWrapper based on pbounds\n",
    "def create_optimizer_wrapper(pbounds):\n",
    "    param_names = list(pbounds.keys())\n",
    "    \n",
    "    def optimizerWrapper(self, **kwargs):\n",
    "        params = {name: kwargs.get(name, None) for name in param_names}\n",
    "        if None in params.values():\n",
    "            raise ValueError(\"All parameters must be provided\")\n",
    "        return specificOptimizer(self, **params)\n",
    "    \n",
    "    return optimizerWrapper\n",
    "\n",
    "# Create the optimizerWrapper function\n",
    "optimizerWrapper = create_optimizer_wrapper(pbounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125b726c-ad45-4c40-870a-5d34b3d0d194",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### General optimizer setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7955b78-5c54-4205-80b8-cf73062fa36b",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OptimizationProblem:\n",
    "    def __init__(self):\n",
    "        self.history = pd.DataFrame()\n",
    "        self.totalNumEvals = 0\n",
    "        self.plot_display_handle = None\n",
    "        self.evals_display_handle = None\n",
    "\n",
    "\n",
    "    def updatePlot(self):\n",
    "        plt.figure()\n",
    "        plotKey = \"maximizeMe\"\n",
    "        plt.plot(np.arange(len(self.history[plotKey])), self.history[plotKey], '-')\n",
    "        \n",
    "        plt.title('Optimization History')\n",
    "        plt.xlabel('Evaluation #')\n",
    "        plt.ylabel(plotKey)\n",
    "        plt.yscale('log')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        \n",
    "        if self.plot_display_handle is None:\n",
    "            self.plot_display_handle = display(plt.gcf(), display_id=True)\n",
    "        else:\n",
    "            update_display(plt.gcf(), display_id=self.plot_display_handle.display_id)\n",
    "        plt.close()\n",
    "\n",
    "    def displayEvals(self):\n",
    "        if self.evals_display_handle is None:\n",
    "            self.evals_display_handle = display(f\"Total Num Evals: {self.totalNumEvals}\", display_id=True)\n",
    "        else:\n",
    "            #Can't use '\\n' in this context. <br> doesn't work either\n",
    "            update_display(f\"Total Num Evals: {self.totalNumEvals}, masterToleranceScaling: {masterToleranceScaling}\", display_id=self.evals_display_handle.display_id)\n",
    "\n",
    "# Attach the function to the class as a method\n",
    "OptimizationProblem.optimizerWrapper = optimizerWrapper\n",
    "\n",
    "# Instantiate the optimization problem\n",
    "problem = OptimizationProblem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636c9a69-ac6f-406c-b7c7-1a0801da07bc",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "# Convert pbounds to the format required by differential_evolution\n",
    "bounds = [(low, high) for (low, high) in pbounds.values()]\n",
    "param_names = list(pbounds.keys())\n",
    "\n",
    "#scipy.optimize wants to optimize a function which is passed a vector of all the parameters\n",
    "#This programmatically wraps the existing wrapper (ugh...) to handle this format\n",
    "def create_DE_wrapper(obj, param_names):\n",
    "    def wrapperDE(params):\n",
    "        param_dict = dict(zip(param_names, params))\n",
    "\n",
    "        #bayes_opt is a maximizer but differential_evolution is a minimizer... hence the inversion\n",
    "        return -1*obj.optimizerWrapper(**param_dict)\n",
    "    \n",
    "    return wrapperDE\n",
    "\n",
    "wrapperDE = create_DE_wrapper(problem, param_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86c1a72-1d2e-4dd7-9056-c34bfa63a40f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "defaultSettingsVector = [importedDefaultSettings[key] for key in list(pbounds.keys())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477c9433-724f-429d-9ea4-54c010f39ffe",
   "metadata": {},
   "source": [
    "## Run optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429f794c-3fd3-4ef4-a4b2-e04c59726519",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "#optimizationType = \"differentialEvolution\" #Classic, standard for exploration\n",
    "optimizationType = \"nelderMead\"            #Classic, standard for refinement\n",
    "#optimizationType = \"L-BFGS-B\"              #This never seems to work; I think the problem is too noisy to get a reasonable Hessian\n",
    "#optimizationType = \"powell\"                #This rarely seems to help; I think the orthogonal approach is not appropriate for this problem\n",
    "#optimizationType = \"COBYQA\"                #Seems promising. Not timid about making big, coordinated moves. I like it more for refinement though. Seems (much) more likely than nelderMead to give up too early though...\n",
    "#optimizationType = \"bayesian\"              #This uses a different library, different signature, different objective, different bounds, etc... beware!\n",
    "\n",
    "match optimizationType:\n",
    "\n",
    "    case \"differentialEvolution\":\n",
    "        totalPopSize = 30            # Total population size\n",
    "        numDimensions = len(bounds)  # Number of parameters\n",
    "        \n",
    "        #initializationType = \"uniform\"\n",
    "        initializationType = \"normal\"\n",
    "        \n",
    "        match initializationType:\n",
    "        \n",
    "            case \"uniform\":\n",
    "                populationDE = np.random.rand(totalPopSize, numDimensions)\n",
    "                for i in range(numDimensions):\n",
    "                    low, high = bounds[i]\n",
    "                    populationDE[:, i] = low + populationDE[:, i] * (high - low)  \n",
    "                    \n",
    "            case \"normal\":\n",
    "                populationDE = np.zeros((totalPopSize, numDimensions))\n",
    "                for i in range(numDimensions):\n",
    "                    low, high = bounds[i]\n",
    "                \n",
    "                    #Define mean based on bounds\n",
    "                    #mean = (high + low) / 2\n",
    "                \n",
    "                    #Optional: Define mean based on defaultSettingsVector\n",
    "                    mean = defaultSettingsVector[i]\n",
    "                    \n",
    "                    std_dev = (high - low) / 20\n",
    "                    populationDE[:, i] = np.random.normal(mean, std_dev, totalPopSize)\n",
    "\n",
    "        #Optional: Add specific points to initial evaluation list\n",
    "        populationDE = np.vstack([[ defaultSettingsVector ], populationDE])\n",
    "        \n",
    "        result = differential_evolution(\n",
    "            wrapperDE, \n",
    "            bounds,\n",
    "            maxiter=500, \n",
    "            disp=True,\n",
    "            polish = False, \n",
    "            init = populationDE\n",
    "        )\n",
    "\n",
    "    case \"nelderMead\":\n",
    "        #Optional: Manually define starting simplex. Need to enable in minimize() options\n",
    "        # initial_simplex = np.tile(defaultSettingsVector, (numDimensions + 1, 1))\n",
    "        # for i in range(1, numDimensions + 1):\n",
    "        #     initial_simplex[i][i - 1] *= 1.1\n",
    "        \n",
    "        result = minimize(\n",
    "            wrapperDE, \n",
    "            defaultSettingsVector,\n",
    "            method = \"Nelder-Mead\",\n",
    "            bounds = bounds,\n",
    "            #options={'initial_simplex': initial_simplex},\n",
    "            options={'adaptive': True}\n",
    "        )\n",
    "\n",
    "    case \"L-BFGS-B\":\n",
    "        result = minimize(\n",
    "            wrapperDE, \n",
    "            defaultSettingsVector,\n",
    "            method = \"L-BFGS-B\",\n",
    "            bounds = bounds,\n",
    "            options = {\"eps\": [1e-4*x for x in defaultSettingsVector]}\n",
    "        )\n",
    "\n",
    "\n",
    "    case \"powell\":\n",
    "        result = minimize(\n",
    "            wrapperDE, \n",
    "            defaultSettingsVector,\n",
    "            method = \"powell\",\n",
    "            bounds = bounds,\n",
    "        )\n",
    "\n",
    "    case \"COBYQA\":\n",
    "        result = minimize(\n",
    "            wrapperDE, \n",
    "            defaultSettingsVector,\n",
    "            method = \"COBYQA\",\n",
    "            bounds = bounds,\n",
    "            options = {\"scale\": True, \"initial_tr_radius\": 1e-1, \"final_tr_radius\": 1e-8, \"disp\": True } #\"*_tr_radius\" appears to constrain the scaled, not raw, variables (if scaling is enabled)\n",
    "        )\n",
    "\n",
    "    case \"bayesian\":\n",
    "        optimizer = bayes_opt.BayesianOptimization(\n",
    "                f=problem.optimizerWrapper,\n",
    "                pbounds=pbounds,\n",
    "                random_state=1,\n",
    "                allow_duplicate_points=True, #2024-04-26 it was whining about this\n",
    "                verbose = 0\n",
    "        )\n",
    "        \n",
    "        #Initial point(s) to check\n",
    "        optimizer.probe(\n",
    "            params={key: importedDefaultSettings[key] for key in pbounds.keys()},\n",
    "            lazy=True,\n",
    "        )\n",
    "        \n",
    "        #Refer to https://bayesian-optimization.github.io/BayesianOptimization/exploitation_vs_exploration.html\n",
    "        #and https://github.com/bayesian-optimization/BayesianOptimization/blob/master/bayes_opt/bayesian_optimization.py\n",
    "        boInitialKappa = 10 + 0*10.\n",
    "        boFinalKappa = 1e-3 + 0*0.1\n",
    "        boNumIter = 1000\n",
    "        boKappaDecay = (boFinalKappa / boInitialKappa)**(1/boNumIter)\n",
    "        \n",
    "        # acquisition_function = bayes_opt.util.UtilityFunction(\n",
    "        #                                    kind='ucb',\n",
    "        #                                    kappa=boInitialKappa,         #Default 2.576\n",
    "        #                                    xi=0.0,              #Default 0\n",
    "        #                                    kappa_decay=boKappaDecay,       #Default 0\n",
    "        #                                    kappa_decay_delay=0  #Default 0                     \n",
    "        #                                 )\n",
    "        \n",
    "        optimizer.maximize(\n",
    "            init_points=100, #Initial, random points. Unwise to omit. \"Often, 2*dim to 5*dim random points are recommended before switching to model-driven exploration.\"\n",
    "            n_iter=boNumIter,\n",
    "            #acquisition_function=acquisition_function\n",
    "        )\n",
    "\n",
    "print(problem.history) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537d5d94-ee16-4a6d-af06-53139a015f2b",
   "metadata": {},
   "source": [
    "## Check out results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50f42b1-5f6b-4a17-99ac-3902f40189bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#problem.history.to_csv('output_data.csv', index=False)\n",
    "\n",
    "problem.history = problem.history.sort_values(by='maximizeMe', ascending=False)\n",
    "\n",
    "\n",
    "bestConfigData = problem.history.iloc[0]\n",
    "bestConfigDict = bestConfigData.to_dict()\n",
    "\n",
    "print( bestConfigData ) \n",
    "\n",
    "\n",
    "\n",
    "setLattice(tao, **bestConfigDict)\n",
    "\n",
    "\n",
    "trackBeam(tao, centerBC14 = True, centerBC20 = True, verbose = True)\n",
    "\n",
    "P = getBeamAtElement(tao, \"PENT\")\n",
    "PDrive, PWitness = getDriverAndWitness(P)\n",
    "\n",
    "print(f\"\"\"P, sigma x: {P[\"sigma_x\"]}\"\"\")\n",
    "print(f\"\"\"PDrive, sigma x: {PDrive[\"sigma_x\"]}\"\"\")\n",
    "print(f\"\"\"PWitness, sigma x: {PWitness[\"sigma_x\"]}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dec3b8-84cd-4210-955d-3ff84b7e60b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bestConfigDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041feb20-cbd4-4875-a53a-27e1673b6381",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(plotMod(P, 'x', 'y',  bins=300))\n",
    "display(plotMod(P, 'x', 'pz', bins=300))\n",
    "display(plotMod(P, 'x', 'xp', bins=300))\n",
    "display(plotMod(P, 'y', 'yp', bins=300))\n",
    "display(plotMod(P, 'delta_t', 'pz', bins=300))\n",
    "display(slicePlotMod(P, 'norm_emit_x',n_slice=100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e145e5-5de1-4039-8a0e-7615383bd0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(plotMod(PWitness, 'x', 'y',  bins=300))\n",
    "display(plotMod(PWitness, 'x', 'pz', bins=300))\n",
    "display(plotMod(PWitness, 'x', 'xp', bins=300))\n",
    "display(plotMod(PWitness, 'y', 'yp', bins=300))\n",
    "display(plotMod(PWitness, 'delta_t', 'pz', bins=300))\n",
    "display(slicePlotMod(PWitness, 'norm_emit_x',n_slice=100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e873d9e9-c62f-40e3-859e-798648f88cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"P, sigma x: {P[\"sigma_x\"]}\"\"\")\n",
    "print(f\"\"\"PDrive, sigma x: {PDrive[\"sigma_x\"]}\"\"\")\n",
    "print(f\"\"\"PWitness, sigma x: {PWitness[\"sigma_x\"]}\"\"\")\n",
    "\n",
    "display(plotMod(P, 'x', 'y',  bins=300))\n",
    "display(plotMod(PDrive, 'x', 'y',  bins=300))\n",
    "display(plotMod(PWitness, 'x', 'y',  bins=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9d5824-6cf6-4e11-8aae-00d366d69c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = getBeamAtElement(tao, \"MFFF\")\n",
    "\n",
    "display(plotMod(P, 'x', 'xp', bins=300))\n",
    "display(slicePlotMod(P, 'norm_emit_x',n_slice=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7c2188-44ab-4d6b-8d52-bbb3c47f758d",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
